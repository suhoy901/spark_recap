{"cells":[{"cell_type":"markdown","source":["## 2.10 종합예제\n- 스파크 데이터프레임의 스키마 정보를 알아내는 스키마 추론(schema inference)과 파일의 첫 Row를 헤더로 지정하는 옵션을 사용해서 csv파일을 읽음"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession \\\n  .builder \\\n  .appName(\"Python Spark SQL basic exmaple\") \\\n  .config(\"spark.some.config.option\", \"some-value\") \\\n  .getOrCreate()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["#.csv(\"FileStore/tables/data/flight-data/csv/2015-summary.csv\")\n  flightData2015 = spark \\\n  .read \\\n  .option(\"inferschema\", \"true\") \\\n  .option(\"header\", \"true\") \\\n  .csv(\"/FileStore/tables/data/flight-data/csv/2015_summary-ebaee.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["- 스칼라와 파이썬에서 사용하는 데이터프레임은 불특정 다수의 로우와 컬럼을 갖음. 로우의 수를 알 수 없는 이유는 자연 연산형태의 프랜스포메이션이기 때문이며, 스파크는 각 컬럼의 데이터 타입을 추론하기 위해 적은 양의 데이터를 읽음. DataFrame의 take액션을 호출하면 그제야 결과를 확인할 수 있음"],"metadata":{}},{"cell_type":"code","source":["flightData2015.take(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: [Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Romania&#39;, count=15),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Croatia&#39;, count=1),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Ireland&#39;, count=344)]</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["- 정수 데이터 타입인 count컬럼을 기준으로 데이터를 정렬하는 트랜스포메이션을 추가하였음. sort는 트랜스포메이션이기 때문에 데이터에 변화가 일어나지 않지만 explain 메서드를 통해 스파크 쿼리의 실행계획을 확인할 수 있음"],"metadata":{}},{"cell_type":"code","source":["# 200개의 셔플 파티션을 생성한 것을 아래에서 확인할 수 있음\nflightData2015.sort(\"count\").explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nSort [count#166 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(count#166 ASC NULLS FIRST, 200), [id=#188]\n   +- *(1) FileScan csv [DEST_COUNTRY_NAME#164,ORIGIN_COUNTRY_NAME#165,count#166] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/flight-data/csv/2015_summary-ebaee.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["- 액션을 호출하면 트랜스포메이션 실행 계획을 시작함. 액션을 시작하기 전 셔플의 출력 파티션 수를 줄임"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partition\", \"5\")\nflightData2015.sort(\"count\").take(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: [Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Singapore&#39;, count=1),\n Row(DEST_COUNTRY_NAME=&#39;Moldova&#39;, ORIGIN_COUNTRY_NAME=&#39;United States&#39;, count=1)]</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["### 2.10.1 DataFrame과 SQL\n- 스파크 SQL을 사용하면 모든 DataFrame을 테이블이나 뷰(임시 테이블)로 등록한 후 SQL쿼리를 사용할 수 있음. CreateOrReplaceTempView 메서드를 사용함"],"metadata":{}},{"cell_type":"code","source":["flightData2015.createOrReplaceTempView(\"flight_data_2015\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["- DataFrame과 SQL 중 간편한 방식으로 트랜스포메이션을 지정할 수 있음. 같은 목적으로 두 종류의 실행 계획을 비교해보면 동일한 기본 실행 계획으로 컴파일된 것을 알 수 있음"],"metadata":{}},{"cell_type":"code","source":["query = \"\"\"\nSELECT DEST_COUNTRY_NAME, count(1)\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\n\"\"\"\n\nsqlWay = spark.sql(query)\ndataFrameWay = flightData2015.groupBy('DEST_COUNTRY_NAME').count()\n\nsqlWay.explain()\nprint(\"=================\")\ndataFrameWay.explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#164], functions=[finalmerge_count(merge count#214L) AS count(1)#202L])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#164, 200), [id=#318]\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#164], functions=[partial_count(1) AS count#214L])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#164] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/flight-data/csv/2015_summary-ebaee.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n=================\n== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#164], functions=[finalmerge_count(merge count#219L) AS count(1)#209L])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#164, 200), [id=#357]\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#164], functions=[partial_count(1) AS count#219L])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#164] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/flight-data/csv/2015_summary-ebaee.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["- 특정 위치를 왕래하는 최대 비햇 횟수를 구하기 위해 max함수를 활용. max함수는 필터링을 수행해 단일 로우를 결과로 반환하는 트랜스포메이션임"],"metadata":{}},{"cell_type":"code","source":["#SQL\nspark.sql(\"SELECT max(count) FROM flight_data_2015\").take(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: [Row(max(count)=370002)]</div>"]}}],"execution_count":15},{"cell_type":"code","source":["#python\nfrom pyspark.sql.functions import max\nflightData2015.select(max('count')).take(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[19]: [Row(max(count)=370002)]</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["- 상위 5개의 도착 국가를 찾아내는 복잡한 코드"],"metadata":{}},{"cell_type":"code","source":["maxSql = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\nORDER BY sum(count) DESC\n\"\"\")\n\nmaxSql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+-----------------+\n DEST_COUNTRY_NAME|destination_total|\n+------------------+-----------------+\n     United States|           411352|\n            Canada|             8399|\n            Mexico|             7140|\n    United Kingdom|             2025|\n             Japan|             1548|\n           Germany|             1468|\nDominican Republic|             1353|\n       South Korea|             1048|\n       The Bahamas|              955|\n            France|              935|\n          Colombia|              873|\n            Brazil|              853|\n       Netherlands|              776|\n             China|              772|\n           Jamaica|              666|\n        Costa Rica|              588|\n       El Salvador|              561|\n            Panama|              510|\n              Cuba|              466|\n             Spain|              420|\n+------------------+-----------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql.functions import desc\n\nflightData2015 \\\n  .groupBy(\"DEST_COUNTRY_NAME\") \\\n  .sum('count') \\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\") \\\n  .sort(desc(\"destination_total\")) \\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+-----------------+\n DEST_COUNTRY_NAME|destination_total|\n+------------------+-----------------+\n     United States|           411352|\n            Canada|             8399|\n            Mexico|             7140|\n    United Kingdom|             2025|\n             Japan|             1548|\n           Germany|             1468|\nDominican Republic|             1353|\n       South Korea|             1048|\n       The Bahamas|              955|\n            France|              935|\n          Colombia|              873|\n            Brazil|              853|\n       Netherlands|              776|\n             China|              772|\n           Jamaica|              666|\n        Costa Rica|              588|\n       El Salvador|              561|\n            Panama|              510|\n              Cuba|              466|\n             Spain|              420|\n+------------------+-----------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["flightData2015 \\\n  .groupBy(\"DEST_COUNTRY_NAME\") \\\n  .sum(\"count\") \\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\") \\\n  .sort(desc('destination_total')) \\\n  .explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nSort [destination_total#373L DESC NULLS LAST], true, 0\n+- Exchange rangepartitioning(destination_total#373L DESC NULLS LAST, 200), [id=#735]\n   +- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#164], functions=[finalmerge_sum(merge sum#377L) AS sum(cast(count#166 as bigint))#369L])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#164, 200), [id=#731]\n         +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#164], functions=[partial_sum(cast(count#166 as bigint)) AS sum#377L])\n            +- *(1) FileScan csv [DEST_COUNTRY_NAME#164,count#166] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/data/flight-data/csv/2015_summary-ebaee.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["- (1)은 파일을 읽음. 그 다음 (1~2)는 groupBy, sum()을 실행, 마지막으로는 정렬과 limit(5)를 실행하는 순서"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"2_IntroductionToSpark","notebookId":4204417576039419},"nbformat":4,"nbformat_minor":0}
