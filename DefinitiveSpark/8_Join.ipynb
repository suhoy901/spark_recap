{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession \\\n  .builder \\\n  .appName(\"Python Spark SQL basic example\") \\\n  .config(\"spark.some.config.option\", \"some-value\") \\\n  .getOrCreate()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"markdown","source":["## 8. Join\n- 선택한 조인 타입에 따라 조인의 성능에 영향을 미치\n- 조인할 때 익스큐터에서 실행 파일 간의 데이터 셔플링을 필요로 하기 때문에 다른 조인과 조인순서를 고려해야함"],"metadata":{}},{"cell_type":"markdown","source":["- 스파크의 조인 타입\n  - inner join, outer join, left outer join, right outer join\n  - left semi join : 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 있는 경우에는 키가 일치하는 왼쪽 데이터셋만 유지\n  - left anti join : 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 없는 경우에는 키가 일치하지 않는 왼쪽 데이터 셋만 유지\n  - natural join : 두 데이터셋에서 동일한 이름을 가진 컬럼을 암시적으로 결합하는 조인을 수행\n  - cross join : 왼쪽 데이터셋의 모든 로우와 오른쪽 데이터 셋의 모든 로우를 조합\n  \n- 주요 조인\n![img](https://miro.medium.com/max/2492/0*jfglHEcbPtwv50J1.png)\n![join_visualize](https://i0.wp.com/www.powerbi-pro.com/wp-content/uploads/2018/05/051118_1848_PowerBIsech1.png?fit=727%2C437&ssl=1)"],"metadata":{}},{"cell_type":"markdown","source":["### 8.1 조인 타입"],"metadata":{}},{"cell_type":"code","source":["# 예제에서 사용할 데이터 셋\nperson = spark.createDataFrame([\n  (0, \"Bill Chamber\", 0, [100]),\n  (1, \"Matel Zaharia\", 1, [500, 250, 100]),\n  (2, \"Michael Arnbrust\", 1, [250, 100])])\\\n.toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n\ngraduateProgram = spark.createDataFrame([\n  (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n  (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n  (1, \"Ph. D\", \"EECS\", \"UC Berkeley\")])\\\n.toDF(\"id\", \"degree\", \"department\", \"school\")\n\nsparkStatus = spark.createDataFrame([\n  (500, \"Vice President\"),\n  (250, \"PMC Member\"),\n  (100, \"Contributor\")])\\\n.toDF(\"id\", \"status\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["person.createOrReplaceTempView(\"person\")\ngraduateProgram.createOrReplaceTempView(\"graduateProgram\")\nsparkStatus.createOrReplaceTempView(\"sparkStatus\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["print(\"person\")\ndisplay(person.limit(20))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th></tr></thead><tbody><tr><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td></tr><tr><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td></tr></tbody></table></div>"]}}],"execution_count":7},{"cell_type":"code","source":["print(\"graduateProgram\")\ndisplay(graduateProgram.limit(20))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>2</td><td>Masters</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"]}}],"execution_count":8},{"cell_type":"code","source":["print(\"sparkStatus\")\ndisplay(sparkStatus.limit(20))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>status</th></tr></thead><tbody><tr><td>500</td><td>Vice President</td></tr><tr><td>250</td><td>PMC Member</td></tr><tr><td>100</td><td>Contributor</td></tr></tbody></table></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["### 8.3 내부 조인 inner join\n- join type 을 명시하지 않았을 때 기본적으로 내부조인을 수행함\n- 왼쪽 테이블과 오른쪽 테이블에서 동일한 컬럼을 가져야함\n- 참으로 평가되는 로우만 결합\n- 세 번째 파라미터로 조인 타입을 명확하게 지정할 수 있음\n- 성능 팁 : 두 테이블의 키가 중복되거나 여러 복사본으로 있다면 성능이 저하됨. 조인이 여러 키를 최소화하기 위해 일종의 카테시안 조인으로 변환되어 버림"],"metadata":{}},{"cell_type":"code","source":["joinExpression = person[\"graduate_program\"] == graduateProgram[\"id\"]\ndisplay(person.join(graduateProgram, joinExpression).limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"]}}],"execution_count":11},{"cell_type":"code","source":["joinExpression = person[\"graduate_program\"] == graduateProgram['id']\ndisplay(person.join(graduateProgram, joinExpression, \"inner\").limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["### 8.4 외부 조인\n- 왼쪽과 오른쪽의 모든 로우를 제공\n- 왼쪽이나 오른쪽 DataFrame에 일치하는 로우가 없다면 해당 위치에 null을 삽입\n- 성능 팁 : 공통 로우가 거의 없는 테이블에서 사용하면 결과값이 매우 커지고 성능이 저하함"],"metadata":{}},{"cell_type":"code","source":["joinExpression = person['graduate_program'] == graduateProgram['id']\ndisplay(person.join(graduateProgram, joinExpression, \"outer\").limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Masters</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["### 8.5 왼쪽 외부 조인\n- 왼쪽의 모든 로우와 오른쪽의 공통 로우(inner join)을 제공. 왼쪽에 일치하는 로우가 없으면 null을 채움"],"metadata":{}},{"cell_type":"code","source":["display(person.join(graduateProgram, joinExpression, \"left_outer\").limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["### 8.6 오른쪽 외부 조인\n- 식의 순서만 바꾸면 왼쪽 외부 조인과 같은 역할\n- 오른쪽의 모든 로우와 왼쪽의 모든 로우(inner join)을 제공. 오른쪽에 일치하는 로우가 없으면 null을 채움\n  - 오른쪽을 기준으로 오늘쪽에 해당하는 값이 왼쪽에 없을 때 왼편에 값을 null로 채움"],"metadata":{}},{"cell_type":"code","source":["display(person.join(graduateProgram, joinExpression, \"right_outer\").limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>2</td><td>Masters</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"]}}],"execution_count":18},{"cell_type":"code","source":["display(graduateProgram.join(person, joinExpression, \"left_outer\").limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>degree</th><th>department</th><th>school</th><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th></tr></thead><tbody><tr><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td></tr><tr><td>2</td><td>Masters</td><td>EECS</td><td>UC Berkeley</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["### 8.7 왼쪽 세미 조인\n- 오른쪽에 존재하는 것을 기반으로 왼쪽 로우만 제공\n- 값이 존재하는지 확인하는 용도, 값이 있다면 왼쪽 DataFrame에 중복 키가 있더라도 로우는 결과에 포함\n- 기존 조인 기능과는 달리 DataFrame의 필터 기능과 유사\n- 성능 : 하나의 테이블만 확실히 고려되고, 다른 테이블은 조인 조건만 확인하기 때문에 성능이 매우 좋음"],"metadata":{}},{"cell_type":"code","source":["joinType = \"left_semi\"\ndisplay(graduateProgram.join(person, joinExpression, joinType).limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"]}}],"execution_count":21},{"cell_type":"code","source":["# 중복 키의 처리\ngradProgram2 = graduateProgram.union(spark.createDataFrame([\n  (0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")\n]))\ngradProgram2.createOrReplaceTempView(\"gradProgram2\")\n\ndisplay(gradProgram2.join(person, joinExpression, joinType).limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>0</td><td>Masters</td><td>Duplicated Row</td><td>Duplicated School</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["### 8.8 왼쪽 안티 조인\n- 왼쪽 세미 조인의 반대 개념. 즉, 오른쪽 DataFrame의 어떤 값도 포함하지 않음\n- SQL의 NOT IN과 같은 스타일의 필터"],"metadata":{}},{"cell_type":"code","source":["display(graduateProgram)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>2</td><td>Masters</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"]}}],"execution_count":24},{"cell_type":"code","source":["joinType = \"left_anti\" # leftanti로 설정해도 됨\ndisplay(graduateProgram.join(person, joinExpression, joinType).limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>2</td><td>Masters</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["### 8.9 자연 조인\n- 조인하려는 컬럼을 암시적으로 추정\n- 암시적인 처리는 언제나 위험하므로 비추천\n- Python join 함수는 이 기능을 지원하지 않음\n\n```SQL\nSELECT * FROM graduateProgram NATURAL JOIN person\n```"],"metadata":{}},{"cell_type":"code","source":["display(spark.sql(\"\"\"\n  SELECT * FROM graduateProgram NATURAL JOIN person\n\"\"\").limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>degree</th><th>department</th><th>school</th><th>name</th><th>graduate_program</th><th>spark_status</th></tr></thead><tbody><tr><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td><td>Bill Chamber</td><td>0</td><td>List(100)</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td></tr><tr><td>2</td><td>Masters</td><td>EECS</td><td>UC Berkeley</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td></tr></tbody></table></div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["### 8.10 교차 조인 cross join(카테시안 조인)\n- 교차 조인은 조건절을 기술하지 않은 내부 조인을 의미\n- 왼쪽의 모든 로우를 오른쪽의 모든 로우와 결합함(결과의 로우수 = 왼쪽 로우의 숫자 * 오른쪽 로우의 숫자)\n- 성능 : 큰 데이터에서 사용할 경우 out-of-memory exception 발생. 가장 좋지 않은 성능을 가진 조인. 주의해서 사용하며 특정 사례에서만 사용해야 함"],"metadata":{}},{"cell_type":"code","source":["# 크로스 조인이지만 조건을 설정해야 하며, 조건에 부합된 결과를 출력하여 inner조인과 동일\n\njoinType = \"cross\"\ndisplay(graduateProgram.join(person, on=joinExpression, how=joinType).limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>degree</th><th>department</th><th>school</th><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th></tr></thead><tbody><tr><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td></tr></tbody></table></div>"]}}],"execution_count":29},{"cell_type":"code","source":["# spark 2.1 이후\ndisplay(person.crossJoin(graduateProgram).limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th><th>id</th><th>degree</th><th>department</th><th>school</th></tr></thead><tbody><tr><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td><td>2</td><td>Masters</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>2</td><td>Masters</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td></tr><tr><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td><td>2</td><td>Masters</td><td>EECS</td><td>UC Berkeley</td></tr><tr><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td></tr></tbody></table></div>"]}}],"execution_count":30},{"cell_type":"code","source":["# spark 2.1 이후부터 : 조인에 조건을 부여하는 것을 잊으면 AnalysisException이 발생함"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["display(person.join(graduateProgram).limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\norg.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plans\nProject [_1#16L AS id#24L, _2#17 AS name#25, _3#18L AS graduate_program#26L, _4#19 AS spark_status#27]\n+- LogicalRDD [_1#16L, _2#17, _3#18L, _4#19], false\nand\nProject [_1#32L AS id#40L, _2#33 AS degree#41, _3#34 AS department#42, _4#35 AS school#43]\n+- LogicalRDD [_1#32L, _2#33, _3#34, _4#35], false\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1413)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1410)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:284)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:284)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:284)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:284)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:268)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1410)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$optimizedPlan$1.apply(QueryExecution.scala:96)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$optimizedPlan$1.apply(QueryExecution.scala:96)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:95)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:95)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$sparkPlan$1.apply(QueryExecution.scala:100)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$sparkPlan$1.apply(QueryExecution.scala:100)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:99)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:99)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:106)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3496)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:2895)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:149)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:54)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$getResultBufferInternal$1.apply(PythonDriverLocal.scala:984)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$getResultBufferInternal$1.apply(PythonDriverLocal.scala:931)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:876)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.getResultBufferInternal(PythonDriverLocal.scala:931)\n\tat com.databricks.backend.daemon.driver.DriverLocal.getResultBuffer(DriverLocal.scala:492)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.com$databricks$backend$daemon$driver$PythonDriverLocal$$outputSuccess(PythonDriverLocal.scala:918)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$repl$6.apply(PythonDriverLocal.scala:364)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal$$anonfun$repl$6.apply(PythonDriverLocal.scala:351)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:876)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.repl(PythonDriverLocal.scala:351)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:396)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:373)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:275)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:373)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)"]}}],"execution_count":32},{"cell_type":"markdown","source":["- AnalysisException을 무시하려면\n  - ```spark.sql.crossJoin.enable=trueSpark``` 세션 빌더 객체에서 설정하거나\n  - ```spark-shell에서 -conf spark.sql.crossJoin.enabled=true```로 변경해야 함\n    - ```scala> spark.sparkContext.getConf.get(\"spark.sql.crossJoin.enable\")```로 속성 설정 확인"],"metadata":{}},{"cell_type":"markdown","source":["**그 외 join에서 고려해야할 사항**\n  - theta 조건을 충족하려면 join되는 [Theta Join - Tutorials point](https://www.tutorialspoint.com/dbms/database_joins.htm): key끼리 매치되도록 버켓을 만듦\n  - 한 테이블의 한 열과 두번째 테이블의 여러 열과 매핑되는 One to Many Join : 한 열의 크기가 작으므로 일반적으로 메모리를 신경쓰지 않아도 됨. parquet을 사용하면 문제가 X\n  - 하나의 테이블이 자신을 조인하는 self join: 데이터의 크기가 클 경우 out-of-memory 발생할 수 있음"],"metadata":{}},{"cell_type":"markdown","source":["### 8.11 조인 사용시의 문제점\n#### 8.11.1 복합 데이터 타입의 조인\n- 불리언을 반환하는 모든 표현식은 조인 표현식으로 간주할 수 있음"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import expr\n\ndisplay(person.withColumnRenamed(\"id\", \"personId\") \\\n  .join(sparkStatus, expr(\"array_contains(spark_status, id)\")).limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>personId</th><th>name</th><th>graduate_program</th><th>spark_status</th><th>id</th><th>status</th></tr></thead><tbody><tr><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td><td>100</td><td>Contributor</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>500</td><td>Vice President</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>250</td><td>PMC Member</td></tr><tr><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td><td>100</td><td>Contributor</td></tr><tr><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td><td>250</td><td>PMC Member</td></tr><tr><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td><td>100</td><td>Contributor</td></tr></tbody></table></div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["#### 8.11.2 중복 컬럼명 처리"],"metadata":{}},{"cell_type":"code","source":["gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\njoinExpr = gradProgramDupe['graduate_program'] == person['graduate_program']"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":38},{"cell_type":"code","source":["# 조인을 수행했음에도 두 개의 gradudate_program 컬림이 존재\ndisplay(gradProgramDupe.join(person, joinExpr, \"inner\").limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>graduate_program</th><th>degree</th><th>department</th><th>school</th><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th></tr></thead><tbody><tr><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td></tr></tbody></table></div>"]}}],"execution_count":39},{"cell_type":"code","source":["# 중복 컬럼 조회 시 오류 발생\ndisplay(gradProgramDupe.join(person, joinExpr, \"inner\").select(\"graduate_program\").limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1349.select.\n: org.apache.spark.sql.AnalysisException: Reference &#39;graduate_program&#39; is ambiguous, could be: graduate_program, graduate_program.;\n\tat org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:247)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$42.apply(Analyzer.scala:937)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$42.apply(Analyzer.scala:939)\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:936)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:945)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:945)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:945)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:1023)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:1023)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:1023)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:948)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:948)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:775)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$1.apply(Dataset.scala:82)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$1.apply(Dataset.scala:80)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3543)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1377)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3456580407407345&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># 중복 컬럼 조회 시 오류 발생</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>display<span class=\"ansi-blue-fg\">(</span>gradProgramDupe<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>person<span class=\"ansi-blue-fg\">,</span> joinExpr<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;inner&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;graduate_program&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>limit<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">select</span><span class=\"ansi-blue-fg\">(self, *cols)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1352</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Alice&#39;</span><span class=\"ansi-blue-fg\">,</span> age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">12</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Bob&#39;</span><span class=\"ansi-blue-fg\">,</span> age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">15</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1353</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1354</span><span class=\"ansi-red-fg\">         </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jcols<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>cols<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1355</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1356</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;Reference &#39;graduate_program&#39; is ambiguous, could be: graduate_program, graduate_program.;&#34;</div>"]}}],"execution_count":40},{"cell_type":"code","source":["# 해결 방법 1 : 다른 조인 표현식 사용\n## 중복된 두 컬럼 중 하나가 자동 제거\ndisplay(person.join(gradProgramDupe, \"graduate_program\").select(\"graduate_program\").limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>graduate_program</th></tr></thead><tbody><tr><td>0</td></tr><tr><td>1</td></tr><tr><td>1</td></tr></tbody></table></div>"]}}],"execution_count":41},{"cell_type":"code","source":["\"\"\" 해결 방법 2: 조인 후 컬럼 제거 \"\"\"\ndisplay(gradProgramDupe.join(person, joinExpr).drop(gradProgramDupe[\"graduate_program\"])\\\n    .select(\"graduate_program\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>graduate_program</th></tr></thead><tbody><tr><td>0</td></tr><tr><td>1</td></tr><tr><td>1</td></tr></tbody></table></div>"]}}],"execution_count":42},{"cell_type":"code","source":["# 해결 방법 3: 조인 전 컬러명 변경(가장 확실한 방법)\nfixed_gradProgram = gradProgramDupe.withColumnRenamed(\"graduate_program\", \"grad_id\")\ndisplay(fixed_gradProgram.join(person, fixed_gradProgram['grad_id'] == person['graduate_program']).limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>grad_id</th><th>degree</th><th>department</th><th>school</th><th>id</th><th>name</th><th>graduate_program</th><th>spark_status</th></tr></thead><tbody><tr><td>0</td><td>Masters</td><td>School of Information</td><td>UC Berkeley</td><td>0</td><td>Bill Chamber</td><td>0</td><td>List(100)</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td><td>1</td><td>Matel Zaharia</td><td>1</td><td>List(500, 250, 100)</td></tr><tr><td>1</td><td>Ph. D</td><td>EECS</td><td>UC Berkeley</td><td>2</td><td>Michael Arnbrust</td><td>1</td><td>List(250, 100)</td></tr></tbody></table></div>"]}}],"execution_count":43},{"cell_type":"markdown","source":["### 8.12 스파크의 조인 수행 방식\n#### 8.12.1 네트워크 통신 전략\n- **셔플조인** : 전체 노드 간 통신을 유발\n- **브로드캐스트 조인** : 최초 테이블을 전체 노드로 복제 후 통신없이 진행\n- 조인을 실행하려면 다층 익스큐터를 사용해 데이터 프레임의 파티션에서 동작\n- 실제 작업과 성능은 조인의 타입과 조인되는 테이터셋의 특성에 따라 다름\n- 크게 **Broadcast hash join, Shuffle Hash Join, Sort Merge Join**이 있음\n- 세 가지 Join 방식을 그림으로 이해 \n  - [ Spark SQL - 3 common joins (Broadcast hash join, Shuffle Hash join, Sort merge join) explained by Ram Ghadiyaram](https://www.linkedin.com/pulse/spark-sql-3-common-joins-explained-ram-ghadiyaram)"],"metadata":{}},{"cell_type":"markdown","source":["### 1. Broadcast Hash Join : SmallSize join Any Size\n- 큰 데이터셋과 이보다 작은 데이터 셋 간의 조인은 큰 데이터셋의 파티션이 있는 모든 익스큐터에 작은 데이터셋이 브로드캐스트되어 수행\n- 기본 값 : spark.sql.autoBroadcastJoinThreshold = 10mb\n  - spark.sql.autoBroadcast.JoinThreshold 는 조인을 수행할 때 모든 작업자 노드에 브로드캐스트되는 테이블의 최대 크기를 구성\n  - 참고\n    - [Spark Doc - performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)\n    - [Does spark.sql.autoBroadcastJoinThreshold work for joins using Dataset's join operator?](https://stackoverflow.com/questions/43984068/does-spark-sql-autobroadcastjointhreshold-work-for-joins-using-datasets-join-op)\n    \n- 수행\n  - 작은 테이블은 드라이버에 모였다가 다시 모든 노드에 복사\n  - 작은 테이블은 메모리에 올라가고\n  - 큰 테이블은 스트림을 통해서 조인 수행\n  - 브로드캐스트 힌트 ```person.join(broadcast(graduateProgram), Seq(\"id\"))```\n    - ```autoBroadcastJoinThreshold```에 관계없이 힌트가 있는 조인 측이 브로드캐스트됨.\n    - 조인의 양쪽에 브로드캐스트 힌트가 있으면 실제 크기가 더 작은 쪽이 브로드 캐스트 됨\n    - 힌트가 없고 테이블의 실제 물리적 추정값이 autoBroadcast.JoinThreshold 보다 작으면, 해당 테이블은 모든 실행기 노드로 브로드캐스트 됨.\n    \n- 성능\n  - 한 쪽의 데이터가 하나의 machine 에 fit-in-memory 될 정도로 작으면 성능 좋음\n    - table broadcast 는 네트워크를 많이 사용하므로, 브로드캐스트 된 테이블일 크면 때때로 out-of-memory 나 성능 저하가 발생할 수 있음\n  - 셔플링이 없기 때문에, 브로드캐스트 되는 쪽이 데이터가 작으면 다른 알고리즘보다 빠름\n  \n\n- 브로드캐스트 지원\n  - Full outer join은 지원하지 않음\n  - left-outer join에서는 오른쪽 테이블만 브로드캐스트, right-outer join에서는 왼쪽 테이블만 브로드캐스트 가능"],"metadata":{}},{"cell_type":"markdown","source":["### Shuffle hash join : MiddleSize join LargeSize\n- 두 테이블 모두 shuffle을 통해 노드에 분산되고\n- 비교적 작은 테이블이 메모리 버퍼에 올라가고\n- 큰 테이블은 스트림을 통해서 조인 수행\n- 파티션이 전체 익스큐터로 분배\n- 셔플은 비용이 많이 듦. 파티션과 셔플 배포가 최적으로 수행되는지 확인하기 위해 로직을 분석하는 것이 중요\n  - 큰 데이터는 join이 필요한 부분만 filtering을 하거나\n  - repartitioning을 고려해야함\n  \n- Map Reduce Fundamentals 유사\n  - Map - 두 개의 서로 다른 Data frames/table\n    - Output key를 join조건에서 필드로 사용\n    - Shuffle - output key로 두 데이터 세트를 섞음\n  - Reduce -join결과\n  \n![](https://camo.githubusercontent.com/e0be0071ea0207a39f4f2c049b449494d60044c0/68747470733a2f2f692e70696e696d672e636f6d2f6f726967696e616c732f34382f34312f38312f34383431383130646437616435303339376435363662386339626562373837352e6a7067)\n\n**성능을 최적화하기 위해선**\n- join할 키가 균등하게 distribute되어있거나\n- parallelism위한 적절한 수의 키가 있을 때\n\n**성능이 나쁜 경우 - 고르지 않은 sharding 및 제한된 parallelism**\n- data skewness 처럼 하나의 단일 파티션이 다른 파티션에 비해 너무 많은 데이터를 가지고 있을 때\n- 각 스테이트에서 50개 키만 셔플할 수 있음 -> 스파크 클러스터가 크면 고른 sharding과 parellelism으로 해결 못함\n\n![](https://camo.githubusercontent.com/005808129f80059f9fe0f14212a95722a2418df9/68747470733a2f2f696d6167652e736c696465736861726563646e2e636f6d2f6f7074696d697a696e67737061726b73716c6a6f696e732d3137303230393136343633312f39352f6f7074696d697a696e672d6170616368652d737061726b2d73716c2d6a6f696e732d31312d3633382e6a70673f63623d31343836363538393137)"],"metadata":{}},{"cell_type":"markdown","source":["### Sort merge join : Large Size join LargeSize\n- 일치하는 조인키를 sort할 수 있고, 브로드캐스트 조인, 셔플 해시 조인에 적합하지 않는 경우에 사용함\n- shuffle hash join과 비교하면, 클러스터에서 데이터 이동(shuffle)을 최소화함\n- 수행\n  - 두 테이블 모두 셔플 및 정렬이 발생하고\n  - 그나자 막은 쪽이 버퍼를 하고 큰 쪽이 스트리밍으로 조인을 수행\n  - partition은 join 작업 전에 조인키 정렬\n- 참고 : [SortMergeJoinExec Binary Physical Operator for Sort Merge Join](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkPlan-SortMergeJoinExec.html)"],"metadata":{}},{"cell_type":"markdown","source":["### BroadcastNetedLoopJoin\n- 적용 : 조인 키가 지정되어 있지 않고 브로드 캐스트 힌트가 있거나 조인의 한쪽이 브로드캐스트 될 수 있고 spark.sql.autoBroadcastJoinThreshold 보다 작은 경우\n- 브로드캐스트 된 데이터 세트가 크면 매우 느릴 수 있으며 OutOfMemoryException을 일으킬 수 있음"],"metadata":{}},{"cell_type":"markdown","source":["## 테이블 크기에 따른 조인 동작 방식\n### 큰 테이블과 테이블 조인\n- 전체 노드 간 통신이 발생하는 셔플 조인이 발생함\n\n### 큰 테이블과 작은 테이블 조인\n- 작은 DataFrame을 클러스터 전체 워커에 복제한 후 통신없이 진행\n- 모든 단일 노드에서 개별적으로 조인이 수행되므로 CPU가 가장 큰 병목 구간이 됨\n- broadcast 함수(힌트)를 통해 브로드캐스트 조인을 설정할 수 있지만 강제할 수는 없음(옵티마이저가 무시 가능)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import broadcast\n\ndisplay(person.join(broadcast(gradProgramDupe), \"graduate_program\").select(\"graduate_program\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>graduate_program</th></tr></thead><tbody><tr><td>0</td></tr><tr><td>1</td></tr><tr><td>1</td></tr></tbody></table></div>"]}}],"execution_count":50},{"cell_type":"markdown","source":["### 작은 테이블과 작은 테이블 조인\n- 스파크가 결정하도록 내버려두는 것이 가장 좋은 선택임"],"metadata":{}},{"cell_type":"markdown","source":["### Check\n- 브로드캐스트 조인을 가능한 사용하고 조인 전에 관련없는 행을 조인 키로 필터링하여 불필요한 데이터 셔플링을 피하는 것이 좋음\n  - 필요한 경우 **spark.sql.autoBroadcastJoinThrehold**를 적절하게 조정\n- sort-merge join이 default이고 대부분의 시나리오에서 잘 수행됨\n  - Shuffle Hash 조인이 Sort-Merge 조인보다 낫다고 확신이 있으면, Sort-Merge Join을 비활성화해서 shuffle hasg join이 수행되도록 하는 것이 좋음\n    - build size가 stream size보다 작으면 Shuffle Hash 조인이 나음\n- unique한 조인키가 없거나 조인키가 없는 조인은 수행비용이 비싸므로 최대한 피하는 것이 좋음"],"metadata":{}},{"cell_type":"markdown","source":["### 그 외 데이터 특성에 따라 고려해야할 것\n- skewed data\n  - [Spark summit rope 2017 - Working with Skewed Data: The Iterative Broadcast - Rob Keevil & Fokko Driesprong](https://www.youtube.com/watch?v=6zg7NTw-kTQ)\n  - [The art of joining in Spark by Andrea Ialenti - Skew it! This is taking forever!](https://towardsdatascience.com/the-art-of-joining-in-spark-dcbd33d693c)\n  - [Optimize the skew in Spark](https://unraveldata.com/common-failures-slowdowns-part-ii/)\n  - [Data Skew and Garbage Collection to Improve Spark Performance](https://dataengi.com/2019/02/06/spark-data-skew-problem/)"],"metadata":{}},{"cell_type":"markdown","source":["## 파티셔닝 Partitioning\n- 어떤 데이터를 어디에 저장할 것인지 제어할 수 있는 기능\n- RDD는 데이터 파티션으로 구성되고 모든 연산은 RDD의 데이터 파티션에서 수행됨\n- 파티션 개수는 RDD 트랜스포메이션 실행할 태스크 수에 직접적인 영향을 줌\n  - 파티션 개수가 너무 적으면 -> 많은 데이터에서 아주 일부의 CPU/코어만 사용함 -> 성능 저하, 클러스터를 제대로 활용 못함\n  - 파티션 개수가 너무 많다면 -> 실제 필요한 것보다 많은 자원을 사용하게 됨 -> 멀티테넌트 환경에서는 자원 부족 현상 발생\n- Partitioner에 의해 RDD 파티셔닝이 실행된 파티셔너는 파티션 인덱스를 RDD 엘리먼트에 할당함"],"metadata":{}},{"cell_type":"code","source":["var_rdd_one = sc.parallelize((1,2,3))\nvar_rdd_one"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[37]: ParallelCollectionRDD[215] at readRDDFromFile at PythonRDD.scala:332</div>"]}}],"execution_count":55},{"cell_type":"code","source":["var_rdd_one.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[39]: 8</div>"]}}],"execution_count":56},{"cell_type":"markdown","source":["## Shuffling\n- 파티셔너가 어떤 파티션을 사용하든 많은 연산이 RDD의 파티션 전체에 걸쳐 데이터 리파티셔닝이 발생함\n  - 새로운 타티션이 생성되거나 파티션이 축소, 병합될 수 있음\n- 리파티셔닝에 필요한 모든 데이터 이동을 셔플링 shuffling이라고 함\n  - 셔플링을 할 때, Disk I/O, Network I/O가 과도하게 발생함\n  - 셔플링은 계산을 동일 익스큐터의 메모리에서 더 이상 진행하지 않고 익스큐터 간에 데이터를 교환함 -> 많은 성능 지연을 초래할 수 있음\n  - 스파크 잡의 실행 프로세스를 결정, 잡이 스테이지로 분할되는 부분에 영향을 미침\n  - 셔플링이 많을수록 스파크 잡이 실행될 때 더 많은 스테이지가 발생하기 때문에 성능에 영향을 미침\n- 리파티셔닝을 유발하는 연산은 **조인**, 리듀스, 그룹핑, 집계 연산이 있음"],"metadata":{}},{"cell_type":"markdown","source":["### 버켓팅 Bucketing\n- Bucketing = pre-(shuffle + sort) inputs on join Keys\n- 각 파일에 저장된 데이터를 제어할 수 있는 또다른 파일 조직화 기법\n- 동일한 버킷 ID를 가진 데이터가 하나의 물리적 파티션에 모두 모여있기 때문에 데이터를 읽을 때 셔플을 피할 수 있음\n- 같은 키로 계속 조인이 발생하는 경우, 일별 누적으로 쌓여가는 테이블을 버킷팅을 하면 효과를 볼 수 있음"],"metadata":{}},{"cell_type":"markdown","source":["d ### Broadcast variable\n- 브로드캐스트 변수는 모든 익스큐터에서 사용할 수 있는 공유 변수\n- 드라이버에서 한 번 생성되면 익스큐터에서만 읽을 수 있음\n- 전체 데이터셋이 스파크 클러스터에서 전파될 수 있어서 익스큐터에서는 브로드캐스ㅡ 변수의 데이터에 접근할 수 있음\n- **익스큐터 내부에서 실행되는 모든 태스크는 모두 브로드캐스 변수에 접근할 수 있음**"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":60}],"metadata":{"name":"8_Join","notebookId":2811094669348212},"nbformat":4,"nbformat_minor":0}
